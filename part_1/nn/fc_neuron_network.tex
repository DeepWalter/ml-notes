\section{Fully Connected Neuron Networks}
% a 2 layer neuron network
\begin{figure}[h]
    \begin{center}
    \begin{tikzpicture}
        % layer 0: input layer
        \begin{scope}[name prefix=layer0-]
            \node at (0, 0) {Layer 0};
            \node[circle, draw] (1) at (0, -1) {$x_1$};
            \node[circle, draw] (2) at (0, -2.5) {$x_2$};
            \node[circle, draw] (3) at (0, -4) {$x_3$};
        \end{scope}

        % layer 1
        \begin{scope}[name prefix=layer1-]
            \node at (3, 0) {Layer 1};
            \node[circle, draw] (1) at (3, -1) {$a^1_1$};
            \node[circle, draw] (2) at (3, -2) {$a^1_2$};
            \node[circle, draw] (3) at (3, -3) {$a^1_3$};
            \node[circle, draw] (4) at (3, -4) {$a^1_4$};
        \end{scope}

        % layer 2
        \begin{scope}[name prefix=layer2-]
            \node at (6, 0) {Layer 2};
            \node[circle, draw] (1) at (6, -1.5) {$a^2_1$};
            \node[circle, draw] (2) at (6, -3.5) {$a^2_2$};
        \end{scope}

        % weights
        \foreach \x in {1, 2, 3} {
            \foreach \y in {1, 2, 3, 4} {
                \foreach \z in {1, 2} {
                    \draw[gray, ->] (layer0-\x) -- (layer1-\y);
                    \draw[gray, ->] (layer1-\y) -- (layer2-\z);
                }
            }
        }
        \draw (layer0-1) edge[red, ->] node[auto] {$w^1_{12}$} (layer1-2);
        \draw (layer0-3) edge[red, ->] node[auto] {$w^1_{32}$} (layer1-2);
        \draw (layer1-1) edge[red, ->] node[auto] {$w^2_{12}$} (layer2-2);
        \draw (layer1-4) edge[red, ->] node[auto] {$w^2_{42}$} (layer2-2);
    \end{tikzpicture}
    \end{center}
    \caption{A 2 Layer Neuron Network}
\end{figure}

\subsection{Notations}
\begin{description}
    \item[$L$] output layer.
    \item[$n_l$] number of neurons in layer $l$. In particular, $n_0 = n$.
    \item[$w^l_{ij}$] weight from the $i$-th neuron in the layer $l-1$ to the $j$-th neuron in the layer $l$.
    \item[$b^l_j$] bias of the $j$-th neuron in layer $l$.
    \item[$a^l_j$] activation of the $j$-th neuron in layer $l$.
    \item[$z^l_j$] raw output of the $j$-th neuron in layer $l$.
    \item[$\sigma^l_j$] activation function of the $j$-th neuron in layer $l$.
    \item[$\V{W}^l$] weight matrix connecting layer $l-1$ to layer $l$, i.e. $(w^l_{ij})$, of dimension 
    $(n_{l-1},\ n_l)$.
    \item[$\V{b}^l$] bias vector of layer $l$, i.e. $(b^l_j)$, of dimension $(1, n_l)$.
    \item[$\V{z}^l$] raw output vector of layer $l$, of dimension $(1, n_l)$.
    \item[$\V{a}^l$] activation vector of layer $l$, of dimension $(1, n_l)$.
    \item[$\V{\sigma}^l$] vector of activation functions of layer $l$, of dimension $(1, n_l)$.
\end{description}

Some basic equations:

\begin{align}
    z^0_j &= a^0_j = x_j\\
    z^l_j &= \sum_k a^{l-1}_{k} w^l_{kj} + b^l_j\quad\forall~l\geq 1\\
    a^l_j &= \sigma^l_j(z^l_j)
\end{align}
The corresponding matrix forms are:

\begin{align}
    \V{z}^0 &= \V{a}^0 = \V{x} = (x_1, x_2, \ldots, x_{n})\\
    \V{z}^l &= \V{a}^{l-1} \V{W}^l + \V{b}^l\\
    \V{a}^l &= \V{\sigma}^l(\V{z}^l)
\end{align}

For a single input example \V{x}, the cost function $C$ should only directly depend on the output layer $L$,
for example $C$ is the square loss function:
\begin{equation}\label{nn_square_loss}
    C = \frac{1}{2} ||\V{a}^L - \V{y}||^2_2
\end{equation}
For a collection of examples, the cost function is the average cost on those examples:
\begin{equation}
    C = \frac{1}{m} \sum_{i=1}^m C(\V{x}^i)
\end{equation}

\subsection{Backpropagation}
First let's consider the case with a single input example. Let $\delta^l_j$ be the error in the $j$-th neuron
in the $l$-th layer, i.e.
\begin{equation}\label{delta_l_j}
    \delta^l_j = \pfrac{C}{z^l_j}
\end{equation}

For the output layer $L$, by definition we have:
\begin{align*}
    \delta^L_j &= \pfrac{C}{z^L_j}\\
               &= \sum_k \pfrac{C}{a^L_k} \pfrac{a^L_k}{z^L_j}\\
               &= \pfrac{C}{a^L_j} \pfrac{\sigma^L_j(z^L_j)}{z^L_j}\\
               &= \pfrac{C}{a^L_j}\cdot (\sigma^L_j)'(z^L_j)
\end{align*}
Let $\V{\delta}^l = {(\delta^l_j)}_{1 \times n_l}$, then the above equation can be written as:
\begin{equation}
    \V{\delta}^L = \nabla_{\V{a}^L}C \odot (\V{\sigma}^L)'(\V{z}^L)
\end{equation}
For example, when $C$ is the square loss~\eqref{nn_square_loss}, $\nabla_{\V{a}^L}C = \V{a}^L - \V{y}$.
\par
We can write $\V{\delta}^l$ in terms of $\V{\delta}^{l+1}$ as following:
\begin{align*}
    \delta^l_j &= \pfrac{C}{z^l_j}\\
               &= \sum_k \pfrac{C}{z^{l+1}_k} \pfrac{z^{l+1}_k}{z^l_j}\\
               &= \sum_k \delta^{l+1}_k \sum_r \pfrac{a^l_r}{z^l_j} w^{l+1}_{rk}\\
               &= \sum_k \delta^{l+1}_k (\sigma^l_j)'(z^l_j) w^{l+1}_{jk}\\
               &= {\left(\V{\delta}^{l+1} \T{(\V{W}^{l+1})}\right)}_{j} (\sigma^l_j)'(z^l_j)
\end{align*}
Its corresponding matrix form is:
\begin{equation}
    \V{\delta}^l = \left(\V{\delta}^{l+1} \T{(\V{W}^{l+1})}\right) \odot (\V{\sigma}^l)'(\V{z}^l)
\end{equation}

Now, let's compute \pfrac{C}{b^l_j}:
\begin{align*}
    \pfrac{C}{b^l_j} &= \sum_k \pfrac{C}{z^l_k} \pfrac{z^l_k}{b^l_j}\\
                     &= \sum_k \delta^l_k \pfrac{b^l_k}{b^l_j}\\
                     &= \delta^l_j
\end{align*}
In shorthand, it can be rewritten as:
\begin{equation}
    \pfrac{C}{\V{b}} = \V{\delta}
\end{equation}

Similarly, we can compute \pfrac{C}{w^l_{ij}}:
\begin{align*}
    \pfrac{C}{w^l_{ij}} &= \sum_k \pfrac{C}{z^l_k} \pfrac{z^l_k}{w^l_{ij}}\\
                        &= \sum_k \delta^l_k \sum_r \pfrac{(a^{l-1}_r w^l_{rk} + b^l_k)}{w^l_{ij}}\\
                        &= \sum_k \delta^l_k a^{l-1}_i \delta_{kj}\\
                        &= a^{l-1}_i \delta^l_j
\end{align*}
In shorthand, it can be rewritten as:
\begin{equation}
    \pfrac{C}{\V{W}} = \V{a}_{\text{in}}\V{\delta}_{\text{out}}
\end{equation}

For simplicity, let's assume that all the activation functions are the same, i.e. $\sigma^l_i = \sigma$, then 
we can write the pseudocode of backpropagation algorithm easily as the following:

\begin{algorithm}
    \caption{Backpropagation}\label{backpropagation}
    \begin{algorithmic}[1]
        \Require $\V{x} = (x_1, x_2, \ldots, x_n)$
        \For{$l = 1$ \algorithmicto $L$}
            \State Compute $\V{z}^l = \V{a}^{l-1} \V{W}^l + \V{b}^l$ and $\V{a}^l = \sigma(\V{z}^l)$.
        \EndFor
        \State Compute $\V{\delta}^l = \nabla_{\V{a}^L}C \odot \V{\sigma}'(\V{z}^L)$
        \Comment{$\nabla_{\V{a}^L}C = \V{a}^L - \V{y}$ if $C$ is square loss.}
        \For{$l=L-1$ \algorithmicto $1$}
            \State Compute $\V{\delta}^l = \left(\V{\delta}^{l+1} \T{(\V{W}^{l+1})}\right) \odot \sigma'(\V{z}^l)$
        \EndFor
        \Ensure $\pfrac{C}{w^l_{ij}} = a^{l-1}_i \delta^l_j$ and $\pfrac{C}{b^l_j} = \delta^l_j$.
    \end{algorithmic}
\end{algorithm}

We are now ready to do the vectorization. Let $\V{x}^i$ and $\V{y}^i$
be the $i$-th example and its output respectively, let
\begin{align*}
    \V{X} &= {(\V{x}^1; \ldots; \V{x}^m)}_{m \times n}\\
    \V{Y} &= {(\V{y}^1; \ldots; \V{y}^m)}_{m \times n_L}
\end{align*}
the input matrix. 
Let $\V{z}^{i, l}, \V{a}^{i, l}, \V{\delta}^{i, l}$ be the raw output, output, error vectors w.r.t.\ the
$i$-th example respectively. Let 
\begin{align*}
    \V{Z}^l &= {(\V{z}^{1, l}; \ldots; \V{z}^{m, l})}_{m \times n_l}\\
    \V{A}^l &= {(\V{a}^{1, l}; \ldots; \V{a}^{m, l})}_{m \times n_l}\\
    \V{\Delta}^l &= {(\V{\delta}^{1, l}; \ldots; \V{\delta}^{m, l})}_{m \times n_l}
\end{align*}
then we have:
\begin{align}
    \V{Z}^l &= \V{A}^{l-1} \V{W}^l + \V{b}^l\\
    \V{A}^l &= \sigma(\V{Z}^l)\\
    \V{\Delta}^L &= \nabla_{\V{A}^L}C \odot \sigma'(\V{Z}^L)\\
    \V{\Delta}^l &= \left(\V{\Delta}^{l+1} \T{(\V{W}^{l+1})}\right) \odot \sigma'(\V{Z}^l)
\end{align}
If $C = \frac{1}{m}\sum_{i=1}^m C(\V{x}^i)$, then
$$\pfrac{C}{b^l_j} = \operatorname{reduce\_mean}(\operatorname{col}_j(\V{\Delta}^l))$$
and
$$\pfrac{C}{w^l_{ij}} = \operatorname{reduce\_mean}(\operatorname{col}_i(\V{A}^{l-1}) \odot 
\operatorname{col}_j (\V{\Delta}^l))$$

\subsection{Cost Functions}


\subsection{Regularizations}
\subsubsection{$L^1$ and $L^2$ Regularizations}

\subsubsection{Dropout}