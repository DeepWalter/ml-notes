Let \dataset\ be the training set where $y_i \in\{-1, +1\}$. The SVM is an algorithm that tries to find a 
hyperplane $\langle\V{w}, \V{x}\rangle + b = 0$ which separate the positive examples from the negative ones. The 
corresponding predictor is $f(\V{x}) = \sign(\langle\V{w}, \V{x}\rangle + b)$.

\section{Hard SVM}
In the case of hard SVM, we assume that the training set is \magenta{linear separable}. Before going any 
further about the ideas of SVM, let's first introduce the concept of the margin of a hyperplane:
% margin of a hyperplane w.r.t. a training set.
\begin{df}[Margin]
    The margin of a hyperplane $\langle\V{w}, \V{x}\rangle + b = 0$ w.r.t.\ a training set  $D$ is defined as
    \begin{equation*}
    \min_i \frac{y_i(\langle\V{w}, \V{x}_i\rangle + b)}{||\V{w}||}
    \end{equation*}
    Obviously, positive margin means the hyperplane classifies $D$ correctly while negative margin means
    there is at least one point on the wrong side.
    When $D$ is correctly classified by the hyperplane, the margin is just the geometric distance between the
    training set and the hyperplane.
\end{df}

% analysis of the hard SVM algorithm
The core idea of (hard) SVM is to find a hyperplane which separates the training set
\textit{with the largest margin}. That is, we hope to solve the following:\marginnote{Obviously, for any 
positive $\lambda$, $(\V{w}, b)$ and $(\lambda\V{w}, \lambda b)$ specify the same hyperplane with the same
``left'' and ``right'' sides.}
\begin{equation}\label{SVM_original}
    \argmax_{\V{w}, b}\min_i \frac{y_i(\langle\V{w}, \V{x}_i\rangle + b)}{||\V{w}||}
\end{equation}
Let
$$\gamma =\min_i \frac{y_i(\langle\V{w}, \V{x}_i\rangle + b)}{||\V{w}||}$$
then the problem~\eqref{SVM_original} becomes:
\begin{equation}
    \argmax_{\V{w}, b} \gamma\;,\quad \st \frac{y_i(\langle\V{w}, \V{x}_i\rangle + b)}{||\V{w}||} \geqslant
     \gamma \quad\forall\ i
\end{equation}
Let $\gamma \gets ||\V{w}||\gamma$, the above problem becomes:
\begin{equation}
    \argmax_{\V{w}, b} \frac{\gamma}{||\V{w}||}\;,\quad \st y_i(\langle\V{w}, \V{x}_i\rangle + b) \geqslant
    \gamma \quad\forall\ i
\end{equation}
Using the linear separable assumption, we know that $\gamma > 0$. Let $\V{w} \gets \dfrac{\V{w}}{\gamma}$
and $b \gets \dfrac{b}{\gamma}$, then the above becomes:
\begin{equation}\label{SVM_argmax}
    \argmax_{\V{w}, b} \frac{1}{||\V{w}||}\;,\quad \st y_i(\langle\V{w}, \V{x}_i\rangle + b) \geqslant 1
    \quad\forall\ i
\end{equation}
which is obviously equivalent to:
\begin{equation}\label{hard_SVM}
    \argmin_{\V{w}, b} \frac{||\V{w}||^2}{2}\;,\quad \st y_i(\langle\V{w}, \V{x}_i\rangle + b) \geqslant 1
    \quad\forall\ i
\end{equation}

% validity of the above analysis.
\begin{thm}
%\begin{sloppypar}
    Assume the training set \dataset\ is \magenta{linear separable}, then there exists a unique hyperplane 
    separating the dataset with the largest margin, which is given by the solution of the above 
    problem~\eqref{hard_SVM}.
%\end{sloppypar}
\end{thm}
\begin{pf}
    The existence part is immediately from the separability assumption.\\
    TODO
\end{pf}

\begin{re}
    From the problem~\eqref{SVM_argmax}, it is clear that if $(\V{w}, b)$ is the separating hyperplane, then
    $\exists~i\ \st y_i(\langle\V{w}, \V{x}_i\rangle + b) = 1$ and the (largest) margin is $\frac{1}{||\V{w}||}$.
    Since the hyperplane is totally determined\footnote{that is, you can remove other points without affecting
    the separating hyperplane.} by those $\V{x}_i$\,s, they are called the \textbf{supporting vectors} of the
    hyperplane.
\end{re}

\section{Soft SVM}
The hard SVM works well when the data set is linear separable, but it behaves poorly on the sets which are not
linear separable since all the restrictions cannot be satisfied at the same time. In order to adapt to the 
non-separable case, we can allow some points to break the restriction, and penalize them in the optimization
target. That is, we can consider the following problem:
\begin{equation}\label{soft_SVM_original}
    \argmin_{\V{w}, b} \frac{||\V{w}||^2}{2} + C\sum_{i=1}^{m}\max\big(0, 1 - y_i(\langle\V{w}, \V{x}_i\rangle
    + b)\big)
\end{equation}
Here, if $y_i(\langle\V{w}, \V{x}_i\rangle + b) < 1$, we add the penalization 
$1 - y_i(\langle\V{w}, \V{x}_i\rangle +b)$ to the 
optimization target; otherwise, no penalization is added. The constant $C > 0$ is the weight that determines 
how much the penalization matters (or how much you can violate the restrictions). For example, if $C = 0$, 
then the penalization dosen't matter and you can violate all the restrictions; if $C = +\infty$, then the 
penalization matters most and you cannot violate any single restriction.\par
If we introduce the slack variabels $\xi_i = \max\big(0, 1 - y_i(\langle\V{w}, \V{x}_i\rangle +b)\big)$, then
the problem~\eqref{soft_SVM_original} becomes:
\begin{equation}\label{soft_SVM}
    \argmin_{\V{w}, b, \V{\xi}} \frac{||\V{w}||^2}{2} + C\sum_{i=1}^{m}\xi_i\quad\st\left\{
    \begin{aligned}
    & y_i(\langle\V{w}, \V{x}_i\rangle + b) \geqslant 1 - \xi_i\\
    & \xi_i \geqslant 0 
    \end{aligned}\right.
    \quad\forall~i
\end{equation}
this is what we called the soft SVM\@.

\section{Duality}

% duality of hard-SVM
Let $h_i(\V{w}, b) = 1 - y_i(\langle \V{w}, \V{x}_i\rangle + b)$, then the hard SVM~\eqref{hard_SVM} becomes
\begin{equation}\label{hard_SVM_reformatted}
\argmin_{\V{w},b} \frac{1}{2}||\V{w}||^2\quad\st h_i(\V{w}, b) \leqslant 0\quad\forall~i
\end{equation}
Its Lagrangian is
\begin{equation}\label{Lagrangian_hard_SVM}
    L(\V{w}, b; \V{\alpha}) = \frac{1}{2}||\V{w}||^2 + \sum_i \alpha_i h_i(\V{w}, b)
\end{equation}
The orignal problem~\eqref{hard_SVM_reformatted} above is equivalent to
$$\argmin_{\V{w}, b}\max_{\V{\alpha}: \alpha_i \geqslant 0} L(\V{w}, b; \V{\alpha})$$
Let $\displaystyle\theta_D(\V{\alpha}) = \min_{\V{w}, b}L(\V{w}, b; \V{\alpha})$, then 
$\nabla_{\V{w}, b} L(\V{w}, b; \V{\alpha}) = 0$ gives
\begin{subequations}
    \begin{align}
    &\V{w} = \sum_i \alpha_i y_i \V{x}_i\label{sub:w}\\
    &\sum_i y_i \alpha_i = 0\label{sub:b}
    \end{align}
\end{subequations}
Substitute equation~\eqref{sub:w} into the Lagrangian~\eqref{Lagrangian_hard_SVM}, we have
\begin{equation}
    \theta_D(\V{\alpha}) = \sum_i \alpha_i - \frac{1}{2}\sum_{i, j}\alpha_i\alpha_j y_i y_j\langle \V{x}_i, 
    \V{x}_j\rangle
\end{equation}
Hence the dual problem $\displaystyle \max_{\V{\alpha}: \alpha_i \geqslant 0} \argmin_{\V{w}, b}
L(\V{w}, b; \V{\alpha}) = \max_{\V{\alpha}: \alpha_i \geqslant 0}\theta_D(\V{\alpha})$ becomes
\begin{equation*}
    \max_{\V{\alpha}}\sum_i \alpha_i - \frac{1}{2}\sum_{i, j}\alpha_i\alpha_j y_i y_j\langle \V{x}_i, \V{x}_j
    \rangle\quad\st \left\{
    \begin{aligned}
    &\sum_i y_i \alpha_i = 0\\
    &\alpha_i \geqslant 0\quad\forall~i
    \end{aligned}\right.
\end{equation*}
or equivalently
\begin{equation}\label{dual_hard_SVM}
    \min_{\V{\alpha}}\frac{1}{2}\sum_{i, j}\alpha_i\alpha_j y_i y_j\langle \V{x}_i, \V{x}_j\rangle -
    \sum_i \alpha_i \quad\st \left\{
    \begin{aligned}
        &\sum_i y_i \alpha_i = 0\\
        &\alpha_i \geqslant 0\quad\forall~i
        \end{aligned}\right.
\end{equation}
Notice that $\frac{1}{2}||\V{w}||^2$ is convex and $h_i$ are affine linear. And the linear separable assumption
guarantees that there is some $(\V{w}, b)$ \st $h_i(\V{w}, b) < 0\;\forall~i$, i.e.\ the inequality 
restrictions are strict. Hence there is a solution
$(\V{w}^*, b^*)$ for the hard SVM~\eqref{hard_SVM}, and a solution $\V{\alpha}^*$ for the dual
problem~\eqref{dual_hard_SVM}. Moreover, they satisfy the KKT condition:
\begin{equation}\label{KKT_hard_SVM}
    \begin{cases}
        &\V{w}^* - \sum_i \alpha_i^* y_i \V{x}_i = 0\\
        &\sum_i y_i \alpha_i^* = 0\\
        & \alpha_i^* \geqslant 0\\
        & h_i(\V{w}^*, b^*) \leqslant 0\\
        & \alpha_i^* h_i(\V{w}^*, b^*) = 0
    \end{cases}
\end{equation}
Notice that not all $\alpha_i^*$ could be $0$ (otherwise $\V{w}^* = 0$, which is not a solution of the hard SVM).
Let $\alpha^*_{i_0} > 0$. Then $\alpha_{i_0}^* h_{i_0}(\V{w}^*, b^*) = 0$ implies
\begin{equation*}
    b^* = y_{i_0} - \sum_i \alpha_i^* y_i \langle \V{x}_i, \V{x}_{i_0}\rangle
\end{equation*}
In summary, we have
\begin{equation}\label{solution_hard_SVM}
    \begin{cases}
        &\V{w}^* = \sum_i \alpha_i^* y_i \V{x}_i\\
        &b^* = y_{i_0} - \sum_i \alpha_i^* y_i \langle \V{x}_i, \V{x}_{i_0}\rangle
    \end{cases}
\end{equation}
\begin{re}
    For those indices $i$ \st $\alpha^*_i > 0$, the corresponding $\V{x}_i$ are the supporting vectors since
    $h_i(\V{w}^*, b^*) = 0$. And it is easy to see that those $\V{x}_i$ with $\alpha^*_i = 0$ do not affect the
    hyperplane.
\end{re}

% duality of soft-SVM
% TODO: supply more details.
Similarly, let $h_i(\V{w}, b, \xi_i) = 1 - \xi_i - y_i(\langle\V{w}, \V{x}_i\rangle + b)$. Then the Lagrangian
of the soft SVM~\eqref{soft_SVM} is
$$L(\V{w}, b, \V{\xi}; \V{\alpha}, \V{\beta}) = \frac{1}{2}||\V{w}||^2 + C\sum_{i=1}^m \xi_i + \sum_{i=1}^m
\alpha_i h_i(\V{w}, b, \xi_i) + \sum_{i=1}^m \beta_i (-\xi_i)$$
The soft SVM~\eqref{soft_SVM} is equivalent to
$$\argmin_{\V{w}, b, \V{\xi}}\max_{\V{\alpha}, \V{\beta}: \alpha_i, \beta_i \geqslant 0}
L(\V{w}, b, \V{\xi}; \V{\alpha}, \V{\beta})$$
Let $\displaystyle\theta_D(\V{\alpha}, \V{\beta}) = \min_{\V{w}, b, \V{\xi}}L(\V{w}, b, \V{\xi}; \V{\alpha}, 
\V{\beta})$, then $\nabla_{\V{w}, b, \V{\xi}} L = 0$ implies
\begin{subequations}
    \begin{align}
    &\V{w} = \sum_i \alpha_i y_i \V{x}_i\label{sub:soft_w}\\
    &\sum_i y_i \alpha_i = 0\label{sub:soft_b}\\
    &\alpha_i + \beta_i = C\label{sub:soft_C}
    \end{align}
\end{subequations}
Hence we have:
\begin{equation}
    \theta_D(\V{\alpha}, \V{\beta}) =  \sum_i \alpha_i - \frac{1}{2}\sum_{i, j}\alpha_i\alpha_j y_i y_j\langle
    \V{x}_i, \V{x}_j\rangle
\end{equation}
which is exactly the same as before. Hence the dual problem of soft SVM~\eqref{soft_SVM} is
\begin{equation}\label{dual_soft_SVM}
    \min_{\V{\alpha}} \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j\langle \V{x}_i, \V{x}_j\rangle -
    \sum_i \alpha_i\quad\st \left\{
    \begin{aligned}
        &\sum_i \alpha_i y_i = 0\\
        &0 \leqslant \alpha_i \leqslant C \quad\forall~i
    \end{aligned}\right.
\end{equation}
Let $\V{w}^*, b^*, \V{\xi}^*$ be the solution to the original soft SVM~\eqref{soft_SVM_original} and 
$\V{\alpha}^*, \V{\beta}^*$ the solution to the dual problem~\eqref{dual_soft_SVM}, then they satifies the KKT
conditions:
\begin{equation}\label{KKT_soft_SVM}
    \begin{cases}
        &\V{w}^* - \sum_i \alpha_i^* y_i \V{x}_i = 0\\
        &\sum_i y_i \alpha_i^* = 0\\
        & a_i^* + \beta_i^* = C\\
        & \alpha_i^* \geqslant 0\\
        & \beta_i^* \geqslant 0\\
        & \xi_i^* \geqslant 0\\
        & h_i(\V{w}^*, b^*, \xi_i^*) \leqslant 0\\
        & \alpha_i^* h_i(\V{w}^*, b^*, \xi_i^*) = 0\\
        & \beta_i^* \xi_i^* = 0
    \end{cases}
\end{equation}
If $\alpha_i^* > 0$, then $h_i(\V{w}^*, b^*, \xi_i^*) = 0$, i.e.\ 
$y_i(\langle\V{w}^*, \V{x}_i\rangle + b^*) = 1 - \xi^*_i$. Those $\V{x}_i$ are called supporting vectors. 
Moreover, if $\alpha^*_i < C$, then $\beta^*_i > 0$, hence $\xi^*_i = 0$ and
$y_i(\langle\V{w}^*, \V{x}_i\rangle + b^*) = 1$, thus those supporting vectors locate on the decision 
boundaries; if $\alpha^*_i = C$, then $\beta^*_i = 0$, thus those supporting vectors locate between the
decision boundaries if $\xi^*_i \leqslant 1$ and they are mis-classified if $\xi^*_i > 1$. If $\alpha^*_i = 0$,
then $\beta^*_i = C$ and $\xi^*_i = 0$, thus $1 \leqslant y_i(\langle\V{w}^*, \V{x}_i\rangle + b^*)$, that is 
they are correctly classified\footnote{but they don't affect the separating hyperplane.}.

If there is some $\alpha_{i_0}$ such that 
$0 < \alpha_{i_0}^* < C$, then the solution of the soft SVM~\eqref{soft_SVM} can be written as:
\begin{equation}
    \begin{cases}
        &\V{w}^* = \sum_i y_i \alpha^*_i \V{x}_i\\
        &b^* = y_{i_0} - \sum_i y_i \alpha^*_i \langle \V{x}_i, \V{x}_{i_0}\rangle
    \end{cases}
\end{equation}

\section{Kernel Method}
The general idea of kernel method is that after mapping the original feature space into an Hilbert space via
a map $\phi$, if we need to compute the inner product \angpair{\phi(\V{x}_i)}{\phi(\V{x}_j)} which usually is
difficult, we hope that there is a kernel function $\kappa$ \st $\angpair{\phi(\V{x}_i)}{\phi(\V{x}_j)} =
\kappa(\V{x}_i, \V{x}_j)$ and the latter is easier to compute. Following this idea, the dual problem~\eqref{dual_hard_SVM} in
Hilbert space
\begin{equation}
    \min_{\V{\alpha}}\frac{1}{2}\sum_{i, j}\alpha_i\alpha_j y_i y_j\langle \phi(\V{x}_i), \phi(\V{x}_j)\rangle
    - \sum_i \alpha_i \quad\st \left\{
    \begin{aligned}
        &\sum_i y_i \alpha_i = 0\\
        &\alpha_i \geqslant 0\quad\forall~i
        \end{aligned}\right.
\end{equation}
becomes
\begin{equation}
    \min_{\V{\alpha}}\frac{1}{2}\sum_{i, j}\alpha_i\alpha_j y_i y_j \kappa(\V{x}_i, \V{x}_j)
    - \sum_i \alpha_i \quad\st \left\{
    \begin{aligned}
        &\sum_i y_i \alpha_i = 0\\
        &\alpha_i \geqslant 0\quad\forall~i
        \end{aligned}\right.
\end{equation}
and its solution satifies
\begin{equation}
    \begin{cases}
        &\V{w}^* = \sum_i \alpha_i^* y_i \phi(\V{x}_i)\\
        &b^* = y_{i_0} - \sum_i \alpha_i^* y_i \kappa(\V{x}_i, \V{x}_{i_0})
    \end{cases}
\end{equation}
Hence the predictor is 
$$y = \sign\left(\sum_i \alpha_i^* y_i \kappa(\V{x}_i, \V{x}) + y_{i_0} - \sum_i \alpha_i^* y_i 
\kappa(\V{x}_i, \V{x}_{i_0})\right)$$

\begin{thm}[Kernel function]
    Let $\mathcal{X}$ be a feature space, $\kappa(\cdot\,, \cdot)$ is some symmetric function on 
    $\mathcal{X} \times \mathcal{X}$, then $\kappa$ is a kernel function if and only if for any data set
    $D = \{\V{x}_1, \dotsc, \V{x}_m\}$, the matrix
    \begin{equation*}
        \begin{bmatrix}
            \kappa(\V{x}_1, \V{x}_1) &\cdots &\kappa(\V{x}_1, \V{x}_j) &\cdots &\kappa(\V{x}_1, \V{x}_m)\\
            \vdots                   &\ddots &\vdots                   &\ddots &\vdots\\
            \kappa(\V{x}_i, \V{x}_1) &\cdots &\kappa(\V{x}_i, \V{x}_j) &\cdots &\kappa(\V{x}_i, \V{x}_m)\\
            \vdots                   &\ddots &\vdots                   &\ddots &\vdots\\
            \kappa(\V{x}_m, \V{x}_1) &\cdots &\kappa(\V{x}_m, \V{x}_j) &\cdots &\kappa(\V{x}_m, \V{x}_m)
        \end{bmatrix}
    \end{equation*}
    is semi-positive.
\end{thm}

\begin{prop}
    Let $\kappa_1$ and $\kappa_2$ be any kernel functions. Then
    \begin{compactenum}
        \item For any $\gamma_1, \gamma_2 > 0$, $\gamma_1 \kappa_1 + \gamma_2 \kappa_2$ is a kernel function.
        \item $\kappa_1 \cdot \kappa_2$ is a kernel function.
        \item For any function $g$, $\kappa(\V{x}, \V{y}) := g(\V{x})\kappa_1(\V{x}, \V{y})g(\V{y})$ is a 
        kernel function.
    \end{compactenum}
\end{prop}
Some useful kernels are:
\begin{compactenum}
    \item Linear kernel: $\kappa(\V{x}_1, \V{x}_2) = \langle \V{x}_1, \V{x}_2 \rangle$.
    \item Polynomial kernel: $\kappa(\V{x}_1, \V{x}_2) = {\langle \V{x}_1, \V{x}_2 \rangle}^d$ where 
    $d \geqslant 1$.
    \item Guassian kernel: $\kappa(\V{x}_1, \V{x}_2) = \exp(-\frac{||\V{x}_1 - \V{x}_2||^2}{2\sigma^2})$ where
    $\sigma > 0$.
    \item Laplacian kernel: $\kappa(\V{x}_1, \V{x}_2) = \exp(-\frac{||\V{x}_1 - \V{x}_2||}{\sigma})$ where
    $\sigma > 0$.
    \item Sigmoid kernel: $\kappa(\V{x}_1, \V{x}_2) = \tanh(\beta \langle \V{x}_1, \V{x}_2\rangle + \theta)$
    where $\beta > 0,\;\theta < 0$.
\end{compactenum}

\section{Support Vector Regression}
Although SVM is used for binary classification, we can apply its idea to linear regression. Namely, we can
tolerate those examples that are close enough to the predictor and only penalize those that are far away.
More specifically, we want to find the predictor $f(\V{x}) = \langle\V{w}, \V{x}\rangle + b$ by minimizing the loss
\marginnote{$L$ is just a regularized loss.}
\begin{equation}\label{SVR_loss}
    L(\V{w}, b) := \frac{1}{2}||\V{w}||^2 + C\sum_{i=1}^m l_\varepsilon(\hat{y}_i - y_i)
\end{equation}
where $l_\varepsilon$ is the $\varepsilon$-insensitive loss:
\begin{equation}\label{epsilon_insensitive_loss}
    l_\varepsilon(z) = 
    \begin{cases}
        0, & \text{if}~ |z| < \varepsilon\\
        |z| - \varepsilon, & \text{otherwise}
    \end{cases}
\end{equation}
Notice that $l_\varepsilon(z) = \max(|z| - \varepsilon, 0) = \max(z - \varepsilon, -z - \varepsilon, 0)$, if
we introduce the slack variabels $\xi_i = l_\varepsilon(\hat{y}_i - y_i)$, the problem becomes
\begin{equation}
    \argmin_{\V{w}, b, \V{\xi}} \frac{1}{2}||\V{w}||^2 + C\sum_{i=1}^m \xi_i \quad\st\quad\left\{
    \begin{aligned}
        &\xi_i \geqslant 0\\
        &\xi_i \geqslant \hat{y}_i - y_i - \varepsilon\\
        &\xi_i \geqslant -\hat{y}_i + y_i - \varepsilon
    \end{aligned}\right.\forany i
\end{equation}
Its Lagrangian is:
\begin{multline}
    L(\V{w}, b, \V{\xi}; \V{\alpha}, \V{\beta}, \V{\gamma}) = \frac{1}{2} ||\V{w}||^2 + C\sum_{i=1}^m \xi_i
    + \sum_{i=1}^m \alpha_i (-\xi_i) + \sum_{i=1}^m \beta_i (\hat{y}_i - y_i -\xi_i -\varepsilon)
    \\+ \sum_{i=1}^m \gamma_i (-\hat{y}_i + y_i - \xi_i -\varepsilon)
\end{multline}
Its dual problem is 
$$\max_{\substack{\V{\alpha}, \V{\beta}, \V{\gamma}\\ \alpha_i, \beta_i, \gamma_i \geqslant 0}}
\min_{\V{w}, b, \V{\xi}} L(\V{w}, b, \V{\xi};\V{\alpha}, \V{\beta}, \V{\gamma})$$
For the inner minimization problem,
$$\nabla_{\V{w}, b, \V{\xi}}L(\V{w}, b, \V{\xi};\V{\alpha}, \V{\beta}, \V{\gamma}) = 0$$
gives
\begin{subequations}
    \begin{align}
        &\V{w} = \sum_i (\gamma_i - \beta_i) \V{x}_i\\
        &\sum_i (\beta_i - \gamma_i) = 0\\
        &C = \alpha_i + \beta_i + \gamma_i
    \end{align}
\end{subequations}
Substitute those equations into the dual problem, we get
\begin{equation*}
    \max_{\substack{\V{\alpha}, \V{\beta}, \V{\gamma}\\ \alpha_i, \beta_i, \gamma_i \geqslant 0}} 
    -\frac{1}{2}\sum_{i, j}(\beta_i - \gamma_i)(\beta_j - \gamma_j)
    \langle\V{x}_i, \V{x}_j\rangle + \sum_i \big(y_i(\gamma_i - \beta_i) - \varepsilon(\gamma_i + \beta_i)\big)
\end{equation*}
which is equivalent to
\begin{equation}
    \min_{\substack{\V{\beta}, \V{\gamma} \\ \beta_i, \gamma_i \geqslant 0}} \frac{1}{2}
    \sum_{i, j}(\beta_i - \gamma_i)(\beta_j - \gamma_j)
    \langle\V{x}_i, \V{x}_j\rangle + \sum_i \big(\varepsilon(\beta_i + \gamma_i) + y_i(\beta_i - \gamma_i)\big)
\end{equation}
subjecting to the conditions
\begin{equation*}
    \begin{cases}
        & \sum_i (\beta_i - \gamma_i) = 0\\
        & \beta_i + \gamma_i \leqslant C\\
        & \beta_i \geqslant 0\\
        & \gamma_i \geqslant 0
    \end{cases}
\end{equation*}
Moreover, the KKT condition is
\begin{equation}
    \begin{cases}
        &\V{w} = \sum_i (\gamma_i - \beta_i) \V{x}_i\\
        &\sum_i (\beta_i - \gamma_i) = 0\\
        &C = \alpha_i + \beta_i + \gamma_i\\
        &\alpha_i, \beta_i, \gamma_i \geqslant 0\\
        &\xi_i \geqslant 0\\
        &\hat{y}_i - y_i - \xi_i - \varepsilon \leqslant 0\\
        &-\hat{y}_i + y_i -\xi_i - \varepsilon \leqslant 0\\
        &\alpha_i \xi_i = 0\\
        &\beta_i (\hat{y}_i - y_i - \xi_i - \varepsilon) = 0\\
        &\gamma_i (-\hat{y}_i + y_i -\xi_i - \varepsilon) = 0
    \end{cases}
\end{equation}
If $\beta_i > 0$, then $\hat{y}_i - y_i -\xi_i - \varepsilon = 0$, from this we can get
\begin{equation}
    b = y_i + \xi_i + \varepsilon - \sum_{j=1}^m(\gamma_j-\beta_j)\langle\V{x}_j, \V{x}_i\rangle
\end{equation}
Similarly, if $\gamma_i > 0$, then $-\hat{y}_i + y_i -\xi_i - \varepsilon = 0$, which implies
\begin{equation}
    b = y_i - \xi_i - \varepsilon - \sum_{i=1}^m (\gamma_j - \beta_j)\langle\V{x}_j, \V{x}_i\rangle
\end{equation}
To get the bias $b$ of the predictor, we can first calculate all $b$\,s by the above two equations, then use
their average as the bias.