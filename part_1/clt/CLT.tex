\section{Computational Learning Theory}
In this section, we mainly consider supervised learning.
Let $\mathcal{X}$ be the instance space, $\mathcal{Y}$ the label set, $D = \{(\V{x}_1, y_1), \ldots, 
(\V{x}_m, y_m)\}$ the training set. Assume $\mathcal{D}$ is the distribution on $\mathcal{X}$, and all 
instances of $D$ are sampled i.i.d.\ according to $\mathcal{D}$. Let \hypo{f} be the underlying labeling 
function and \hypo{h} any prediction function, then the \textbf{true (generalization) loss (error)} is defined
as:
$$L_{\mathcal{D}, f}(h) := \p_{\V{x}\sim\mathcal{X}}(h(\V{x})\neq f(\V{x})) := \mathcal{D}\left(\{\V{x}: h(\V{x})
\neq f(\V{x})\}\right)$$
the \textbf{empirical risk (error, loss)} is defined as:
$$L_S(h) = \frac{1}{m}\sum_{i=1}^m \indi(h(\V{x})\neq f(\V{x}))$$

\subsection{Probably Approximately Correct Learning}

% concept class.
\begin{df}[Concept class]
    A (target) concept is just a true labeling function \hypo{c}, that is, for any instace $(\V{x}, y)$ 
    (assuming sampling process is noise free) we have $c(\V{x}) = y$. The collection $\mathcal{C}$ of all 
    target concepts is called the concept class.
\end{df}

% hypothesis space.
\begin{df}[Hypothesis space]
    The collection of all labeling functions \hypo{f} a learner $\mathcal{L}$ can return is called the 
    hypothesis space (w.r.t.\ $\mathcal{L}$). We denote it as $\mathcal{H}$.
\end{df}

% inductive bias.
\begin{re}[Inductive bias]
    By restricting our learner to the hypothesis space instead of arbitrary predictors, we bias it toward a 
    particular set of predictors. Such restrictions are called \textbf{inductive bias}.
\end{re}

% realizability assumption.
\begin{df}[Realizability Assumption]
    The realizable assumption asserts that there is a $h^* \in \mathcal{H}$ s.t.\ 
    $L_{\mathcal{D}, f}(h^*) = 0$.
\end{df}

\begin{re}
    The realizable assumption implies that with probability 1 over i.i.d.\ samples $D$, we have $L_D(h^*) = 0$.
    That is, $\mathcal{D}^m(\{D: L_D(h^*) = 0\}) = 1$.
\end{re}

% PAC learnability.
\begin{df}[PAC learnability]\label{PAC_learnability}
    The concept class $\mathcal{C}$ is PAC learnable w.r.t.\ a hypothesis space $\mathcal{H}$ if there exist
    \begin{enumerate}
        \item a function $m: {(0, 1)}^2 \longrightarrow \mathbb{N}$;
        \item a learner $\mathcal{L}$.
    \end{enumerate}
    s.t.\ for any $\varepsilon, \delta \in (0, 1)$, for any distribution $\mathcal{D}$ over $\mathcal{X}$, and
    for any concept \hypo{c}, if the realizable assumption holds w.r.t.\ $\mathcal{H}, \mathcal{D}, c$, then
    when applying the learner $\mathcal{L}$ to $m \geq m(\varepsilon, \delta)$ i.i.d.\ samples generated by
    $\mathcal{D}$ and labeled by $c$, the learner returns a hypothesis $h$ s.t.\ with probability at least 
    $1 - \delta$ (over the choice of the samples), we have $L_{\mathcal{D},c}(h) \leq \varepsilon$. That is,
    $$\mathcal{D}^m(\{D: L_{\mathcal{D},c}(h) \leq \varepsilon\}) \geq 1 - \delta$$
\end{df}

% sample complexity.
\begin{df}[Sample complexity]
    The sample complexity of a learner is the minimal number of examples needed for the learner to produce a 
    PAC solution on any i.i.d.\ data sets with that many samples. That is, it is the minimum of all 
    $m(\varepsilon, \delta)$ where $m$ satisfies the requirements in definition~\ref{PAC_learnability}.
\end{df}

% agnostic PAC learnability.