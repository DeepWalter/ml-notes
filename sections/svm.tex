\section{Support Vector Machine}
Let \dataset\ be the training set where $y_i \in\{-1, +1\}$. The SVM is an algorithm that tries to find a 
hyperplane $y = \T{\V{w}}\V{x} + b$ which separate the positive examples from the negative ones. The 
corresponding predictor is $f(\V{x}) = \sign(\T{\V{w}}\V{x} + b)$.

\subsection{Hard SVM}
First we introduce the concept of the margin of a hyperplane:
% margin of a hyperplane w.r.t. a training set.
\begin{df}[Margin]
    The margin of a hyperplane w.r.t.\ a training set is the minimal Euclidean distance between the point in the 
    training set and the hyperplane, that is:
    \begin{equation*}
    \min_i \frac{|\T{\V{w}}\V{x}_i + b|}{||\V{w}||}
    \end{equation*}
\end{df}

% analysis of the hard SVM algorithm
The core idea of (hard) SVM is to find a hyperplane which separates the training set
\textit{with the largest margin}. That is, we hope to solve the following:
\begin{equation}\label{SVM_original}
    \argmax_{\V{w}, b}\min_i \frac{|\T{\V{w}}\V{x}_i + b|}{||\V{w}||}
\end{equation}
Using the assumption that the hyperplane should correctly separates the training set, let
$$\gamma_i = \frac{y_i(\langle\V{w}, \V{x}_i\rangle + b)}{||\V{w}||}$$
and $\gamma =\displaystyle \min_i \gamma_i$~, then the problem~\eqref{SVM_original} becomes:
\begin{equation}
    \argmax_{\V{w}, b} \gamma\;,\quad \st \frac{y_i(\langle\V{w}, \V{x}_i\rangle + b)}{||\V{w}||} \geq \gamma
    \quad\forall\ i
\end{equation}
Let $\gamma \gets ||\V{w}||\gamma$, the above problem becomes:
\begin{equation}
    \argmax_{\V{w}, b} \frac{\gamma}{||\V{w}||}\;,\quad \st y_i(\langle\V{w}, \V{x}_i\rangle + b) \geq \gamma
    \quad\forall\ i
\end{equation}
Using the linear separable assumption again, we know that $\gamma > 0$. Let $\V{w} \gets \frac{\V{w}}{\gamma}$
and $b \gets \frac{b}{\gamma}$, then the above becomes:
\begin{equation}
    \argmax_{\V{w}, b} \frac{1}{||\V{w}||}\;,\quad \st y_i(\langle\V{w}, \V{x}_i\rangle + b) \geq 1
    \quad\forall\ i
\end{equation}
which is obviously equivalent to:
\begin{equation}\label{hard_SVM}
    \argmin_{\V{w}, b} \frac{||\V{w}||^2}{2}\;,\quad \st y_i(\langle\V{w}, \V{x}_i\rangle + b) \geq 1
    \quad\forall\ i
\end{equation}

% validity of the above analysis.
\begin{thm}
    Assume the training set \dataset\ is linear separable, then there exists a unique hyperplane separating
    the dataset with the largest margin, which is given by the solution of the above 
    problem~\eqref{hard_SVM}.
\end{thm}
\begin{pf}
    The existence part is immediately from the separability assumption.\\
    TODO
\end{pf}

\subsection{Soft SVM}

\subsection{Duality}

\subsection{Kernel method}